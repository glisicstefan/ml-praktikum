# Supervised Learning - Overview

Supervised learning je vrsta maÅ¡inskog uÄenja gde model uÄi od **labeled podataka** - svaki ulaz (features) ima odgovarajuÄ‡i izlaz (label). Model pravi predikcije, poredi ih sa pravim vrednostima, i iterativno smanjuje greÅ¡ku.

**KljuÄna razlika:**
```
Supervised:   X (features) + y (labels) â†’ Model uÄi pattern â†’ Predikcija
Unsupervised: X (features) samo        â†’ Model traÅ¾i strukture
```

---

## Classification vs Regression

| Aspekt | Classification | Regression |
|--------|----------------|------------|
| **Output** | KategoriÄka vrednost | Kontinuirana numeriÄka vrednost |
| **Primeri** | Spam/Ham, Bolest (Da/Ne), Sentiment | Cena kuÄ‡e, Temperatura, Prihod |
| **Algoritmi** | Logistic, SVM, Random Forest, XGBoost | Linear, Ridge, Random Forest, XGBoost |
| **Metrike** | Accuracy, Precision, Recall, F1, AUC | MAE, MSE, RMSE, RÂ² |

---

## Algoritmi PoreÄ‘enje

### Classification

| Algoritam | Brzina | Accuracy | Interpretacija | Scaling | Best For |
|-----------|--------|----------|----------------|---------|----------|
| **Logistic Regression** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | âœ… Required | Binary classification, baseline |
| **Decision Tree** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | âŒ Not needed | Interpretability, EDA |
| **Random Forest** | â­â­â­ | â­â­â­â­â­ | â­â­ | âŒ Not needed | Structured data, robust baseline |
| **XGBoost/LightGBM** | â­â­ | â­â­â­â­â­ | â­â­ | âŒ Not needed | **Best performance**, Kaggle |
| **SVM** | â­â­ | â­â­â­â­ | â­â­ | âœ… **MUST** | Small datasets, high-dim, clear margin |
| **KNN** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | âœ… **MUST** | Small datasets, simple baseline |
| **Naive Bayes** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | âŒ Not needed | **Text classification**, real-time |

### Regression

| Algoritam | Interpretacija | Najbolji Za |
|-----------|----------------|-------------|
| **Linear Regression** | â­â­â­â­â­ | Linear relationships, baseline |
| **Ridge/Lasso** | â­â­â­â­ | Regularization, feature selection |
| **Random Forest** | â­â­ | Non-linear, robust |
| **XGBoost** | â­â­ | Best performance |
| **SVR** | â­â­ | Small datasets, non-linear |

---

## Decision Framework: Koji Algoritam?
```
START
  â”‚
  â”œâ”€> Text classification?
  â”‚     â””â”€ YES â†’ Naive Bayes â†’ Logistic Regression â†’ SVM
  â”‚
  â”œâ”€> Dataset size?
  â”‚     â”œâ”€ Small (<1k)      â†’ Logistic / SVM / KNN
  â”‚     â”œâ”€ Medium (1k-100k) â†’ Random Forest / XGBoost
  â”‚     â””â”€ Large (>100k)    â†’ Logistic / LightGBM
  â”‚
  â”œâ”€> Interpretability critical?
  â”‚     â””â”€ YES â†’ Logistic Regression / Decision Tree
  â”‚
  â”œâ”€> Need best performance?
  â”‚     â””â”€ YES â†’ XGBoost / LightGBM / Ensemble
  â”‚
  â””â”€> Not sure?
        â””â”€ Start: Logistic (linear) + Random Forest (non-linear)
           Compare and iterate
```

---

## Evaluation Metrike

### Classification

| Metrika | Formula | Kada Koristiti |
|---------|---------|----------------|
| **Accuracy** | (TP+TN) / Total | Balanced classes |
| **Precision** | TP / (TP+FP) | Cost of false positives high (spam) |
| **Recall** | TP / (TP+FN) | Cost of false negatives high (cancer) |
| **F1-Score** | 2 Ã— (PÃ—R) / (P+R) | Imbalanced classes |
| **ROC-AUC** | Area under curve | Overall model quality |

### Regression

| Metrika | Interpretacija | Range |
|---------|----------------|-------|
| **MAE** | Average absolute error | [0, âˆž), lower better |
| **RMSE** | Penalizes large errors more | [0, âˆž), lower better |
| **RÂ²** | Variance explained | [0, 1], higher better |

---

## Supervised Learning Workflow
```
1. DATA COLLECTION
   â””â”€ Collect labeled data (X, y)

2. DATA PREPROCESSING
   â”œâ”€ Handle missing values
   â”œâ”€ Encode categorical features
   â”œâ”€ Scale features (if needed)
   â””â”€ Train/test split (80/20)

3. MODEL SELECTION
   â””â”€ Choose algorithm based on problem

4. TRAINING
   â””â”€ Fit model on training data

5. EVALUATION
   â”œâ”€ Test on unseen data
   â””â”€ Calculate metrics

6. HYPERPARAMETER TUNING
   â””â”€ GridSearch / RandomSearch / Optuna

7. DEPLOYMENT
   â””â”€ Serve model in production
```

---

## Folder Content (02_Supervised_Learning)

**Lekcije:**
1. âœ… **Linear Regression** - Osnova regression-a, linear relationships
2. âœ… **Logistic Regression** - Binary i multiclass classification
3. âœ… **Decision Trees** - Interpretable, overfitting prone
4. âœ… **Random Forest** - Ensemble bagging, robust baseline
5. âœ… **Gradient Boosting** - XGBoost/LightGBM/CatBoost, best performance
6. âœ… **Support Vector Machines** - Kernel trick, clear margins
7. âœ… **K-Nearest Neighbors** - Instance-based, simple
8. âœ… **Naive Bayes** - Probabilistic, excellent for text
9. âœ… **Algorithm Comparison** - Side-by-side benchmarks, decision guides

---

## Quick Selection Guide

**Problem â†’ Algorithm:**

| Your Situation | Recommended Algorithm |
|----------------|----------------------|
| Binary classification, need interpretability | Logistic Regression |
| Structured data, want good performance quickly | Random Forest |
| Kaggle competition, need best accuracy | XGBoost / LightGBM |
| Text classification (spam, sentiment) | Naive Bayes |
| Small dataset (<1k), high-dimensional | SVM |
| Need fast predictions, simple baseline | KNN |
| Understand how model decides | Decision Tree |

---

## Key Concepts

**Overfitting vs Underfitting:**
- **Overfitting:** Model memorizes training data (high train acc, low test acc)
- **Underfitting:** Model too simple (low train acc, low test acc)
- **Solution:** Cross-validation, regularization, more data

**Bias-Variance Tradeoff:**
- **High Bias:** Too simple model (underfitting)
- **High Variance:** Too complex model (overfitting)
- **Goal:** Balance both (sweet spot)

**Feature Scaling:**
- **Required:** Logistic, SVM, KNN, Neural Networks
- **Not needed:** Tree-based (Decision Tree, Random Forest, XGBoost)

**Cross-Validation:**
- Don't trust single train/test split
- Use k-fold CV (k=5 or 10)
- More reliable performance estimate

---

## Pros & Cons

**âœ… Prednosti:**
- Jasna struktura i cilj uÄenja
- Lako merljive performanse (metrike)
- OdliÄne performanse sa kvalitetnim labeled podacima
- Variety of algorithms za razliÄite probleme

**âŒ Mane:**
- Zahteva velike koliÄine **labeled podataka** (skupo, time-consuming)
- Model uÄi samo ono Å¡to vidi u training data
- Ne moÅ¾e generalizovati van distribucije training data
- Labeling errors propagiraju u model

---

## Summary Table

| Aspekt | Classification | Regression |
|--------|----------------|------------|
| **Output Type** | Discrete categories | Continuous values |
| **Best Algorithms** | XGBoost, Random Forest, Logistic | XGBoost, Random Forest, Linear |
| **Baseline** | Logistic Regression | Linear Regression |
| **Key Metric** | F1-Score (imbalanced), Accuracy (balanced) | RMSE, RÂ² |
| **Common Use Cases** | Spam detection, fraud, diagnosis | Price prediction, forecasting |

---

**Key Takeaway:** Supervised learning je **najÄeÅ¡Ä‡i ML pristup** u praksi. Za veÄ‡inu problema: poÄni sa **baseline** (Logistic/Linear), probaj **Random Forest** za non-linear patterns, i upgrade na **XGBoost** za best performance. **Feature engineering** i **hyperparameter tuning** Äesto daju veÄ‡i boost od switching algoritama! ðŸŽ¯